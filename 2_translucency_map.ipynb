{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from logging import warning\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_NOTEBOOK: bool = True  # @param {type: \"boolean\"}\n",
    "TRANSLUCENCY_MAP_FILE_NAME: str = \"albedo.png\"  # @param {type: \"string\"}\n",
    "\n",
    "PATH_PREFIX: str = (\n",
    "    \"https://raw.githubusercontent.com/YertleTurtleGit/photometric-stereo-mappings/main/test_dataset/\"\n",
    "    if IS_NOTEBOOK\n",
    "    else \"./../test_dataset/\"\n",
    ")\n",
    "\n",
    "BACK_LIGHT_IMAGE_PATHS: List[str] = [\n",
    "    PATH_PREFIX + str(index) + \"b.png\" for index in range(1, 8 + 1)\n",
    "]\n",
    "MASK_PATH = PATH_PREFIX + \"output/opacity.png\"\n",
    "\n",
    "OUTPUT_PATH = None if IS_NOTEBOOK else PATH_PREFIX + \"output/\" + TRANSLUCENCY_MAP_FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_image(\n",
    "    image_path: str, color: bool = True, target_dtype: np.dtype = np.dtype(\"float64\")\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Reads an image from URI and converts it to an array with specified bit depth.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the image file.\n",
    "        color (bool, optional): Read image as color image. Defaults to True.\n",
    "        target_dtype (np.dtype, optional): The target bit depth. Defaults to np.dtype(\"float64\").\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The output array with shape (w,h,3) for color or (w,h) for grayscale images.\n",
    "    \"\"\"\n",
    "    image = io.imread(image_path)\n",
    "    image_dtype: np.dtype = image.dtype\n",
    "    image = image.astype(target_dtype)\n",
    "\n",
    "    if image_dtype == np.dtype(\"uint8\"):\n",
    "        image /= pow(2, 8) - 1\n",
    "    elif image_dtype == np.dtype(\"uint16\"):\n",
    "        image /= pow(2, 16) - 1\n",
    "    elif image_dtype == np.dtype(\"uint32\"):\n",
    "        image /= pow(2, 32) - 1\n",
    "\n",
    "    if color:\n",
    "        if len(image.shape) == 3:\n",
    "            return image\n",
    "        elif len(image.shape) == 2:\n",
    "            return np.array([image, image, image])\n",
    "        elif len(image.shape) == 4:\n",
    "            return np.array([image[:, :, 0], image[:, :, 1], image[:, :, 2]])\n",
    "        else:\n",
    "            warning(\n",
    "                \"Image channel count of \"\n",
    "                + str(len(image.shape))\n",
    "                + \" with shape \"\n",
    "                + str(image.shape)\n",
    "                + \" is unknown: \"\n",
    "                + image_path\n",
    "            )\n",
    "    else:\n",
    "        if len(image.shape) == 2:\n",
    "            return image\n",
    "        elif len(image.shape) == 3 or len(image.shape) == 4:\n",
    "            return (image[:, :, 0] + image[:, :, 1] + image[:, :, 2]) / 3\n",
    "        else:\n",
    "            warning(\n",
    "                \"Image channel count of \"\n",
    "                + str(len(image.shape))\n",
    "                + \" with shape \"\n",
    "                + str(image.shape)\n",
    "                + \" is unknown: \"\n",
    "                + image_path\n",
    "            )\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translucency_map(back_light_images_paths: List[str], output_path: str, mask_path: str = None):\n",
    "    \"\"\"Computes the translucency mapping given the back light images and saves it to the output path if supplied.\n",
    "    Uses the exposure fusion algorithm.\n",
    "    Mertens, Tom, Jan Kautz, and Frank Van Reeth. \"Exposure fusion.\" 15th Pacific Conference on Computer Graphics and Applications (PG'07). IEEE, 2007.\n",
    "    Args:\n",
    "        back_light_images_paths (List[str]): Paths of back light images from top lighting.\n",
    "        output_path (str): The path where the resulting albedo map is saved.\n",
    "        mask_path (str, optional): Path of the mask that gets applied on the albedo map.\n",
    "    \"\"\"\n",
    "\n",
    "    light_images = [\n",
    "        np.clip(_read_image(image_path) * 255, 0, 255).astype(\"uint8\")\n",
    "        for image_path in back_light_images_paths\n",
    "    ]\n",
    "\n",
    "    merge_mertens = cv.createMergeMertens()\n",
    "    translucency_map = merge_mertens.process(light_images)\n",
    "    translucency_map = np.clip(translucency_map * 255, 0, 255).astype(\"uint8\")\n",
    "\n",
    "    if mask_path:\n",
    "        mask_image = _read_image(mask_path, color=False)\n",
    "        translucency_map[mask_image == 0] = (0, 0, 0)\n",
    "\n",
    "    if output_path:\n",
    "        cv.imwrite(output_path, translucency_map)\n",
    "    else:\n",
    "        plt.imshow(translucency_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    translucency_map(BACK_LIGHT_IMAGE_PATHS, OUTPUT_PATH, MASK_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9fb47bfb27aa80605ee8bc9c35db619ba1a83d93aaa1ab9d0d517b5218a2e478"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
